{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syft[udacity]\n",
      "  Using cached https://files.pythonhosted.org/packages/ba/f1/f1d00ca775eb15bbeb3c5eba85ef4473005adb03e745f7d2c10d45524831/syft-0.2.6.tar.gz\n",
      "Collecting Flask~=1.1.1 (from syft[udacity])\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl\n",
      "Collecting Pillow~=6.2.2 (from syft[udacity])\n",
      "  Using cached https://files.pythonhosted.org/packages/12/ad/61f8dfba88c4e56196bf6d056cdbba64dc9c5dfdfbc97d02e6472feed913/Pillow-6.2.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting aiortc==0.9.28 (from syft[udacity])\n",
      "  Using cached https://files.pythonhosted.org/packages/1a/34/d9c8e19b4d5157721a5b77750116c6bb6355f1d85b92e7de491269b9ee51/aiortc-0.9.28.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-Bl4Byu/aiortc/setup.py\", line 9, in <module>\n",
      "        with open(about_file, encoding=\"utf-8\") as fp:\n",
      "    TypeError: 'encoding' is an invalid keyword argument for this function\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-Bl4Byu/aiortc/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install 'syft[udacity]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adopted Toy model from\n",
    "+ https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2012%20-%20Train%20an%20Encrypted%20Neural%20Network%20on%20Encrypted%20Data.ipynb\n",
    "+ SMPC and homomorphic encryption provide mechanisms to enable computation on encrypted data, without decrypting the underlying values themselves. As a result, data remains encrypted in memory, in process and at rest.\n",
    "+ https://baffle.io/blog/homomorphic-and-multiparty-computation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Set everything up\n",
    "hook = sy.TorchHook(torch) \n",
    "\n",
    "alice = sy.VirtualWorker(id=\"alice\", hook=hook)\n",
    "bob = sy.VirtualWorker(id=\"bob\", hook=hook)\n",
    "james = sy.VirtualWorker(id=\"james\", hook=hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([25])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> [PointerTensor | me:75382377266 -> bob:15890766040]\n",
       "\t-> [PointerTensor | me:32947143687 -> alice:1167520626]\n",
       "\t-> [PointerTensor | me:9749565363 -> james:87315513379]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_x = x.share(bob, alice, james)\n",
    "encrypted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_x.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'covid-19-271622'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_quality</th>\n",
       "      <th>Allele</th>\n",
       "      <th>A1_decrypted</th>\n",
       "      <th>A2_decrypted</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>HLA-A*68:01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result_quality       Allele A1_decrypted A2_decrypted  A3  A4  A5  A6  A7  \\\n",
       "0               1  HLA-A*02:01            0            0   0   0   0   0   0   \n",
       "1               0  HLA-A*68:01            1            1   5   1  13  11  11   \n",
       "\n",
       "   A8  A9  A10  \n",
       "0   0   0    0  \n",
       "1  20  12    0  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $project_id \n",
    "SELECT result_quality, Allele, \n",
    "AEAD.DECRYPT_STRING((SELECT keyset FROM `encryption.encryption_key` as keys where keys.ligand_id=mhc.ligand_id), \n",
    "mhc.A1_encrypted, CAST(mhc.ligand_id AS STRING)) A1_decrypted, \n",
    "AEAD.DECRYPT_STRING((SELECT keyset FROM `encryption.encryption_key` as keys where keys.ligand_id=mhc.ligand_id), \n",
    "mhc.A2_encrypted, CAST(mhc.ligand_id AS STRING)) A2_decrypted, \n",
    "A3, A4, A5, A6, A7, A8, A9, A10 \n",
    "FROM `corona.mhc_qual_feature_encryption` as mhc\n",
    "WHERE ligand_id = 1717014 OR ligand_id = 1025027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]])\n",
    "target = torch.tensor([[0],[0],[1],[1.]])\n",
    "\n",
    "# A Toy Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none-encrypted list(bob._tensors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:78: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n",
      "/opt/conda/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:90: UserWarning: Default args selected\n",
      "  warnings.warn(\"Default args selected\")\n"
     ]
    }
   ],
   "source": [
    "# We encode everything\n",
    "data = data.fix_precision().share(bob, alice, crypto_provider=james, requires_grad=True)\n",
    "target = target.fix_precision().share(bob, alice, crypto_provider=james, requires_grad=True)\n",
    "model = model.fix_precision().share(bob, alice, crypto_provider=james, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:25737328515 -> bob:23320114442]\n",
      "\t-> [PointerTensor | me:64819730123 -> alice:50650138937]\n",
      "\t*crypto provider: james*\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:25737328515 -> bob:23320114442]\n",
      "\t-> [PointerTensor | me:64819730123 -> alice:50650138937]\n",
      "\t*crypto provider: james*\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:34747258875 -> bob:83402176022]\n",
      "\t-> [PointerTensor | me:84918744592 -> alice:14118335382]\n",
      "\t*crypto provider: james*\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:93093756967 -> bob:39277013840]\n",
      "\t-> [PointerTensor | me:93578185764 -> alice:88575155504]\n",
      "\t*crypto provider: james*\n"
     ]
    }
   ],
   "source": [
    "print(model(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2960)\n",
      "tensor(1.1550)\n",
      "tensor(0.6120)\n",
      "tensor(0.2710)\n",
      "tensor(0.1550)\n",
      "tensor(0.0880)\n",
      "tensor(0.0540)\n",
      "tensor(0.0330)\n",
      "tensor(0.0220)\n",
      "tensor(0.0150)\n",
      "tensor(0.0110)\n",
      "tensor(0.0090)\n",
      "tensor(0.0070)\n",
      "tensor(0.0060)\n",
      "tensor(0.0070)\n",
      "tensor(0.0040)\n",
      "tensor(0.0050)\n",
      "tensor(0.0030)\n",
      "tensor(0.0040)\n",
      "tensor(0.0030)\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(params=model.parameters(),lr=0.1).fix_precision()\n",
    "\n",
    "for iter in range(20):\n",
    "    # 1) erase previous gradients (if they exist)\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # 2) make a prediction\n",
    "    pred = model(data)\n",
    "\n",
    "    # 3) calculate how much we missed\n",
    "    loss = ((pred - target)**2).sum()\n",
    "\n",
    "    # 4) figure out which weights caused us to miss\n",
    "    loss.backward()\n",
    "\n",
    "    # 5) change those weights\n",
    "    opt.step()\n",
    "\n",
    "    # 6) print our progress\n",
    "    print(loss.get().float_precision())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adopted Model from\n",
    "+ https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2011%20-%20Secure%20Deep%20Learning%20Classification.ipynb\n",
    "+ Might need to install -> !pip install torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Might need to install -> pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch) \n",
    "client = sy.VirtualWorker(hook, id=\"client\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "n_test_batches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 50\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 100\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/home/jupyter/data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/home/jupyter/data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "private_test_loader = []\n",
    "for data, target in test_loader:\n",
    "    private_test_loader.append((\n",
    "        data.fix_precision().share(alice, bob, crypto_provider=crypto_provider),\n",
    "        target.fix_precision().share(alice, bob, crypto_provider=crypto_provider)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.301686\n",
      "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.209247\n",
      "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.247478\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.187667\n",
      "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.124999\n",
      "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.091852\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.104337\n",
      "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.057239\n",
      "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.133403\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.107137\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.110678\n",
      "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.023410\n",
      "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.121223\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.411033\n",
      "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.062868\n",
      "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.055777\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.079457\n",
      "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.034083\n",
      "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.029965\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.037514\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.034719\n",
      "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.051971\n",
      "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.021198\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.030943\n",
      "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.030483\n",
      "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.028690\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.044513\n",
      "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.119162\n",
      "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.059276\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.070239\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.106075\n",
      "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.071613\n",
      "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.008936\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.027529\n",
      "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.027300\n",
      "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.021255\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.030475\n",
      "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.098288\n",
      "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.248048\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.042589\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.123363\n",
      "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.072707\n",
      "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.006780\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.008656\n",
      "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.023194\n",
      "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.066323\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.068210\n",
      "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.005059\n",
      "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.088119\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.049857\n",
      "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.004114\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.055397\n",
      "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.025428\n",
      "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.020161\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.115592\n",
      "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.056723\n",
      "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.037275\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.005719\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.031006\n",
      "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.022473\n",
      "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.003897\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.006215\n",
      "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.013425\n",
      "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.016884\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.054983\n",
      "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.002666\n",
      "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.072758\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.004978\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.006044\n",
      "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.007223\n",
      "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.026737\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.015110\n",
      "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.016474\n",
      "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.012373\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.001251\n",
      "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.003085\n",
      "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.005650\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.008308\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.001235\n",
      "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.010466\n",
      "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.005522\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.012453\n",
      "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.022192\n",
      "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.007213\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.002861\n",
      "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.019993\n",
      "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.007447\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.003174\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.008226\n",
      "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.005894\n",
      "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.013469\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.003863\n",
      "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.068449\n",
      "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.021219\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.014247\n",
      "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.012429\n",
      "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.002630\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.014343\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
